{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOUysjMn6Cky"
      },
      "source": [
        "Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUPa2Bv_6Ck0"
      },
      "source": [
        "BERT integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NjKoL2xa6Ck1"
      },
      "outputs": [],
      "source": [
        "#pip install torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iH88oct26Ck3"
      },
      "outputs": [],
      "source": [
        "#pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SaZG2D-f6Ck4"
      },
      "outputs": [],
      "source": [
        "#pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0wWPlJ706Ck4"
      },
      "outputs": [],
      "source": [
        "#pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Xcw3bXhG6Ck4"
      },
      "outputs": [],
      "source": [
        "#pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "E7WgqTqW6Ck5"
      },
      "outputs": [],
      "source": [
        "#pip install AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D1Pl8VZf6Ck5"
      },
      "outputs": [],
      "source": [
        "#pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KHQY_wte6Ck6"
      },
      "outputs": [],
      "source": [
        "#pip install tf-keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TeHVz366Ck6"
      },
      "source": [
        "distilbert-base-uncased because:\n",
        "- pre-trained BERT model\n",
        "- text classification\n",
        "- distilled BERT model for smaller dataset\n",
        "- slightly lesser performance than bert-base-uncased\n",
        "- switch to bert-base-uncased for higher accuracy and if we have more computational resources; it will be slower"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_LWecRg6Ck7"
      },
      "source": [
        "maybe not BERT? Any BERT models max text length is 512 and the news texts are longer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HIhCZBa_6Ck7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import torch\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# from datasets import Dataset\n",
        "from transformers import AutoTokenizer # Hugging Face Transformers\n",
        "from transformers import AutoModel\n",
        "#from transformers import TFAutoModelForSequenceClassification\n",
        "#from tensorflow.keras.optimizers import Adam\n",
        "#from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KFYLBbtn6Ck8"
      },
      "outputs": [],
      "source": [
        "news_df = pd.read_csv('fake_or_real_news.csv') # TODO: Get cleaned data instead\n",
        "news_df = news_df.drop(columns=['text'])\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "news_df['label'] = label_encoder.fit_transform(news_df['label'].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qTxsryNK6Ck9"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(news_df, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\") # Example news data (replace with your actual dataset)\n",
        "inputs = tokenizer(news_df['title'].to_list(), return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "epoch_accuracy = []\n",
        "\n",
        "for epoch in range(3):\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "\n",
        "    avg_embedding_magnitude = torch.norm(cls_embeddings, dim=1).mean().item()\n",
        "    epoch_accuracy.append(avg_embedding_magnitude)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}: Average CLS Embedding Magnitude = {avg_embedding_magnitude}\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(epoch_accuracy) + 1), epoch_accuracy, marker='o', linestyle='-', color='b')\n",
        "plt.title(\"CLS Embedding Change Over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Average CLS Embedding Magnitude\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS1RQjOY7sPZ",
        "outputId": "a2c60877-53ca-4cd4-f20b-5aa07343d4fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSnuyQk96Ck9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW\n",
        "# Load the pre-trained tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\") # Example news data (replace with your actual dataset)\n",
        "inputs = tokenizer(news_df['title'].to_list(), return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "model.train()  # Set the model to training mode\n",
        "\n",
        "#here is where I am going to make the loop for the chart of epochs\n",
        "\n",
        "epoch_accuracy = []\n",
        "for epoch in range(3):\n",
        "    optimizer.zero_grad()  # Zero out previous gradients\n",
        "    # Forward pass: Get model outputs\n",
        "    with torch.no_grad(): # Since we're just extracting embeddings, no need to compute gradients\n",
        "        outputs = model(**inputs) # Extract the CLS token embeddings (usually the first token)\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWrF86FM6Ck-"
      },
      "source": [
        "Using logistic regression for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DK-t7CVI6Ck-"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(cls_embeddings, news_df['label'].to_list(), test_size=0.2, random_state=42)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000) # TODO: try knn\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohNyvYKm6Ck-"
      },
      "source": [
        "Using KNN-3 for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J0Vqx1M6Ck_"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    cls_embeddings,\n",
        "    news_df['label'].to_list(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}